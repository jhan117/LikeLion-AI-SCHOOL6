# 05-18

- [실습](#실습)
  - [서울특별시 다산 콜센터의 주요 민원](#서울특별시-다산-콜센터의-주요-민원)

## 실습

### 서울특별시 다산 콜센터의 주요 민원

목록을 스크래핑함과 더불어 내용까지 스크래핑하는 것을 배운다.

1. 모듈 가져오기
   - requests
   - pandas
   - bs4
   - time

```python
import requests
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup as bs
import time
```

2. URL 가져오기 : Network -> Doc -> Headers -> Request URL, Method 확인 (저작권 확인 필수!)

```python
# items_per_page : 몇 개씩 볼 것인지 (기본 : 15개) -> 반복문 덜 쓸 수 있음!
# Max 값이 정해지지 않은 경우는 한 페이지로 임의로 설정할 수 있다.
base_url = "https://opengov.seoul.go.kr/civilappeal/list?items_per_page=50&page=2"
```

3. HTML 읽기

pandas.read_html : table 태그를 읽어 온다.

```python
# 인코딩 시 'cp949' or 'euc_kr'는 error 발생
# -> 파일이 'UTF-8'로 저장되어 있어서 그럼
# Response Headers -> Content-Type: text/html; charset=UTF-8 확인!
table = pd.read_html(base_url, encoding="UTF-8")[0]
```

4. 상세 정보 가져오기

requests -> BeautifulSoup : a 태그의 href 속성에서 내용을 가져온다.

```python
# 요청
response = requests.get(base_url)

# bs로 가져오기
html = bs(response.text, 'lxml')

# table에 있는 링크만 가져오기
# 또는 copy selector를 해서 지정해주는 것도 좋다.
a_list = html.table.select("a")

# 그 링크의 숫자만 가져오기
a_link_no = []
for link in a_list:
    a_link_no.append(link["href"].split("/")[-1])

# 내용 번호 컬럼 설정하기
table["내용번호"] = a_link_no

```

5. 한 페이지 수집 구현

```python
def get_one_page(page_no):
    """
    특정 페이지를 받아서 그 페이지의 50개씩 출력해준다.
    """
    try:
        # 페이지에 내용이 없음에도 에러가 발생하지 않는 경우에 예외처리를 해줘야 한다.
        # 이 때, shape 이용해 column과 row의 수를 비교해서 하던가 아니면 비어있는 값이면 예외처리 해주면 된다.
        url = f"https://opengov.seoul.go.kr/civilappeal/list?items_per_page=50&page={page_no}"

        # 목록 만들기
        df = pd.read_html(url, encoding="UTF-8")[0]

        # 링크 가져오기
        response = requests.get(url)
        html = bs(response.text, 'lxml')

        a_list = html.table.select("a")

        a_link_no = []
        for link in a_list:
            a_link_no.append(link["href"].split("/")[-1])

        # 컬럼 추가
        df["내용번호"] = a_link_no
    except:
        raise Exception (f"{page_no}를 찾을 수 없다")
        # return f"{page_no}를 찾을 수 없다"

    return df
```

6. 여러 페이지 수집 구현

```python
page_no = 1

table_list = []

while True:
    df = get_one_page(page_no)
    page_no += 1

    if df.empty:
        break

    table_list.append(df)
    time.sleep(0.1)
```

7. 데이터 병합

```python
df = pd.concat(table_list)
```

8. csv 파일로 저장 후 읽기 : 파일명도 저장해주기

```python
file_name = "seoul-120-list.csv"

# 저장
df.to_csv(file_name, index=False)

# 읽기
pd.read_csv(file_name)
```
