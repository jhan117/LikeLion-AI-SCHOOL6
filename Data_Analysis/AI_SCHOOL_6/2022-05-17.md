# 05-17

## 실습

### 네이버 금융 뉴스 기사 수집

```python
import time
import pandas as pd
from tqdm import trange

def get_url(item_code, page_no):
    """
    item_code, page_no 를 넘기면 url 을 반환하는 함수
    """

    url = f"https://finance.naver.com/item/news_news.nhn?code={item_code}&page={page_no}&sm=title_entity_id.basic&clusterId="

    return url

def get_one_page_news(item_code, page_no):
    """
    get_url 에 item_code, page_no 를 넘겨 url 을 받아오고
    뉴스 한 페이지를 수집하는 함수
    1) URL 을 받아옴
    2) read_html 로 테이블 정보를 받아옴
    3) 데이터프레임 컬럼명을 ["제목", "정보제공", "날짜"]로 변경
    4) temp_list 에 데이터프레임을 추가
    5) concat 으로 리스트 병합하여 하나의 데이터프레임으로 만들기
    6) 결측치 제거
    7) 연관기사 제거
    8) 데이터프레임 반환
    """

    url = get_url(item_code, page_no)

    table = pd.read_html(url, encoding = "cp949")

    news_list = []
    cols = table[0].columns
    for news in table[:-1]:
        news.columns = cols
        news_list.append(news)

    df = pd.concat(news_list)
    df = df.dropna()
    df = df[~df["제목"].str.contains("연관기사")]

    return df

code = "005930"
name = "삼성전자"
file_name = f"news_{code}_{name}.csv"

news_list = []
for page in trange(1, 11):
    news_list.append(get_one_page_news(code, page))
    time.sleep(1)

df_news = pd.concat(news_list, ignore_index=True)

df_news.to_csv(file_name)
pd.read_csv(file_name)
```

### 네이버 금융 개별 종목 수집

```python
import pandas as pd
import requests
from bs4 import BeautifulSoup as bs

def get_html(item_code, page_no):
    """
    웹 스크래핑
    """
    url = f"https://finance.naver.com/item/sise_day.naver?code={item_code}&page={page_no}"
    headers = {'user-agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)

    return response.text

def get_day_list(item_code, page_no):
    """
    일자별 시세를 페이지별로 수집
    """
    response = get_html(item_code, page_no)

    html = bs(response, 'lxml')
    table = pd.read_html(str(html.select("table")), encoding="cp949")

    return table[0].dropna()

def get_item_list(item_code, item_name):
    """
    일별 시세를 수집하는 함수
    """
    page = 0
    item_list = []

    response = get_html(item_code, page_no)

    html = bs(response, 'lxml')
    last_page =html.select("a")[-1]['href'].split("=")[-1]

    while True:
        page += 1
        df = get_day_list(item_code, page)
        item_list.append(df)
        time.sleep(0.1)

        if page == int(last_page):
            df_item = pd.concat(item_list, ignore_index=True)
            item_date = df_item.loc[0, "날짜"]

            file_name = f"{item_name}_{item_code}_{item_date}.csv"
            df_item.to_csv(file_name, index=False)
            break

    return pd.read_csv(file_name)

get_item_list("323410", "카카오뱅크")
```
